{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfjUACWbk5S9"
      },
      "source": [
        "# Logistic regression from a neural network perspectives\n",
        "\n",
        "In this notebook we will see how to implement logistic regression from a neural network perspective using the `python` library `Keras` (https://keras.io/)\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Some concepts are assumed to be known (at least partially) and will not be covered:\n",
        "\n",
        "- splitting the data in the **training**, **validation** and **test** sets (cross-validation and overfitting)\n",
        "- binary classification problems and logistic regression\n",
        "- basic Python\n",
        "\n",
        "However, the material can be used withouth knowing the above.\n",
        "\n",
        "## Keras\n",
        "https://drive.google.com/file/d/1KeEVvLRZztK5tEQG0wmhRAiFumlD2kmo/view?usp=sharing\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1KeEVvLRZztK5tEQG0wmhRAiFumlD2kmo\">\n",
        "\n",
        "- Large community of users\n",
        "- Multi-backend, multi-platform\n",
        "- Easy and quick development and deployment of deep learning models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Grc7C6Csi_j"
      },
      "source": [
        "### Steps in a Keras implementation of deep learning\n",
        "\n",
        "1. **set up**: import libraries and configure the environment (e.g. set seed)\n",
        "2. **data management**: load data and preprocessing, training and test sets\n",
        "3. **define hyperparameters**: n. of nodes, n. of layers, type of layers, activation function, loss function, optimiser, n. of epochs (deep learning architecture)\n",
        "4. **build the model**: define the topology of the network and build the stack of layers\n",
        "5. **compile the model**: compile the model by combining the defined layers, the loss function, the optimiser and the chosen metrics for evaluation\n",
        "6. **train the model**: fit the deep learning model for the chosen n. of epochs (iterations)\n",
        "7. **predictions**: use the trained model to obtain predictions\n",
        "8. **evaluate the model**: use test data to measure how well the model did (loss, accuracy metrics)\n",
        "9. **'rinse and repeat'**: modify and repeat until satisfied (e.g. change hyperparameters, model architecture, network topology etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3p7FBiqmouB"
      },
      "source": [
        "## Set up\n",
        "\n",
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPlZ98cNOJne"
      },
      "source": [
        "## import libraries\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcFPST72mukG"
      },
      "source": [
        "## The data\n",
        "\n",
        "The dataset we are going to use is very famous. It was published by Ronald Fisher in 1936 together with the paper [The use of multiple measurements in taxonomic problems](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x). Data are public and nowadays this dataset is shipped with many statistical software and packages. We are going to use the copy coming with [sci-kit learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFcGq29OWV9"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "iris.data = pd.DataFrame(iris.data, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(iris.target) #converting numpy array -> pandas Series\n",
        "iris.target = iris.target.to_frame() #converting Pandas series to dataframe\n",
        "print('Shape of the feature table: ' + str(iris.data.shape))\n",
        "print('Shape of the target array: ' + str(iris.target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX_5VYT_R5Td"
      },
      "source": [
        "print(iris.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPMpJVxgc-cM"
      },
      "source": [
        "print(iris.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9tllXkdRYD1"
      },
      "source": [
        "features = iris.data.iloc[:,:]\n",
        "target = iris.target\n",
        "\n",
        "#change these two values to plot different features, remembering the numbering:\n",
        "# 0 : sepal length (cm)\n",
        "# 1 : sepal width (cm)\n",
        "# 2 : petal length (cm)\n",
        "# 3 : petal width (cm)\n",
        "feature_x = 0\n",
        "feature_y = 1\n",
        "\n",
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in two bunches\n",
        "ax.scatter(x=iris.data.iloc[0:50,feature_x],    y=iris.data.iloc[0:50,feature_y],    c='red',   label=iris.target_names[0])\n",
        "ax.scatter(x=iris.data.iloc[50:100,feature_x],  y=iris.data.iloc[50:100,feature_y],  c='green', label=iris.target_names[1])\n",
        "ax.scatter(x=iris.data.iloc[100:150,feature_x], y=iris.data.iloc[100:150,feature_y], c='blue',  label=iris.target_names[2])\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOWIuMHigcdQ"
      },
      "source": [
        "We are now using all four features of the `iris` dataset: `sepal length (cm)`, `sepal width (cm)`, `petal length (cm)` and `petal width (cm)`.\n",
        "We first reduce the problem to binary classification, by merging two classes together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai_XE2gkOd5z"
      },
      "source": [
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "n1 = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setos+versicolor vs virginica\n",
        "target[0:n1] = 0 \n",
        "target[n1:150] = 1\n",
        "\n",
        "print(iris.target.iloc[:,0].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJCWlvUNtR7d"
      },
      "source": [
        "As before, a visual check of the two classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BPh7_4vOgW5"
      },
      "source": [
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in two bunches\n",
        "ax.scatter(x=features.iloc[0:n1,0],   y=features.iloc[0:n1,1],   c='red',  label='Non virginica')\n",
        "ax.scatter(x=features.iloc[n1:150,0], y=features.iloc[n1:150,1], c='blue', label='Virginica')\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAOwHJmxO1-D"
      },
      "source": [
        "### Neural Network model\n",
        "\n",
        "In the slides we saw first how to implement logistic regression as a simple neural network model with just the output layer: this output layer had only one node (binary classification) which performed both the regression and sigmoid activation steps:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "We are now building a **shallow neural network model**, by adding **one hidden layer** with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjRDH3RhvcJ7"
      },
      "source": [
        "#### Training and test sets\n",
        "\n",
        "We first prepare the training and test set for correct evaulation of the neural network model performance: we make one split (one training set and one test set), and assign 80% of the data to the training set and 20% of the data to the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMPW3k6_OlVd"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target.iloc[train_index,:]\n",
        "    target_val     = target.iloc[val_index,:]\n",
        "    \n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(\"Training features size: \", features_train.shape)\n",
        "print(\"Test features size: \", features_val.shape)\n",
        "print(\"Training targets size: \", target_train.shape)\n",
        "print(\"Test targets size: \", target_val.shape)\n",
        "\n",
        "print(\"Type of the training features object: \", type(features_train))\n",
        "print(\"Type of the training targets object: \", type(target_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1OFhmMsik-D"
      },
      "source": [
        "print(target_train.iloc[:,0].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKSJsYcCivI_"
      },
      "source": [
        "print(target_val.iloc[:,0].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwsSzWgnwgFL"
      },
      "source": [
        "## Shallow neural network models\n",
        "\n",
        "#### Building the neural network model\n",
        "\n",
        "We now build the (shallow) **neural network model** using the `Keras` framework: we choose a `sequential` architecture and add `dense` (fully connected) layers (**1 hidden layer**, **1 output layer**).\n",
        "\n",
        "We have to set a number of `hyperparameters` (the **building blocks** -or 'ingredients'- of a neural network model):\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWTGCvZbPAk"
      },
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features \n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IbRkmOS-HHZ"
      },
      "source": [
        "We chose `ReLU` activation functions for the units in the hidden layer, and `sigmoid` activation function for the output layer.\n",
        "\n",
        "We chose <a href='https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a'>binary cross-entropy</a> for binary classification problems: this is the loss function we have been discussing in the slides.\n",
        "As for the optimizer, we use `SDG`: more details on the available otimizers in Keras can be found <a href='https://keras.io/api/optimizers/'>here</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQDDNpS_czT6"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXVGYykpZXPp"
      },
      "source": [
        "### Binary classification\n",
        "\n",
        "<!-- We revise here the classification exercises on the `iris` dataset moving from logistic regression to shallow neural networks. -->\n",
        "Initially, we'll address binary classification. \n",
        "\n",
        "We first import some libraries and the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHMzMrHVOpB4"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will \n",
        "#flow like INPUT -> ELABORATION -> OUTPUT.\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes.\n",
        "from keras.layers import Dense\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4O2yFSjc-Wh"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfZaLf2L0jI3"
      },
      "source": [
        "The `summary()` method of the Keras model tells us that there are 49  parameters to train:\n",
        "- w1, w2, w3, w4, b (weights for the 4 features + bias term) for each of the 8 nodes in the hidden layer ($\\rightarrow$ 40 parameters);\n",
        "- w1 - w8 + b (weights for the 8 nodes results + bias term) for the output layer ($\\rightarrow$ 9 parameters) [&#161; you may remember from the matrix dimensions we discussed in the slides !]\n",
        "\n",
        "#### Training the neural network model\n",
        "\n",
        "We have now prepared everything we need and are ready to train the model on our data. It's an iterative process that cycles many times through what are called `epochs` (~ iterations). We'll start with using the parameter set above:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0UlsKrDdCiK"
      },
      "source": [
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fclNIr25eTGi"
      },
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym491pXNlplI"
      },
      "source": [
        "#### Confusion matrix\n",
        "\n",
        "With more than two features we can't plot the decision boundary; however, we can have an idea of the performance of our classification model by looking at the <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a>:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "biUNLb-tyrOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRZFu9myaV-A"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(features_val)\n",
        "print(predictions)\n",
        "predicted_labels = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_labels = target_val.to_numpy().reshape((len(target_val),1))\n",
        "target_labels = np.where(target_labels > 0.5, \"virginica\", \"non-virginica\")\n",
        "print(target_labels)\n",
        "con_mat_df = confusion_matrix(target_labels, predicted_labels, labels=[\"non-virginica\",\"virginica\"])\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWK_HCChKYk"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijg0GxZYzBO6"
      },
      "source": [
        "We can now calculate the **overall accuracy** of the model, and the **TPR** (true positive rate) and the **TNR** (true negative rate):\n",
        "\n",
        "$$\n",
        "\\text{TPR}=\\frac{TP}{TP+FN}\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{TNR}=\\frac{TN}{TN+FP}\\\\\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy_z4Jzyv3jC"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(target_labels, predicted_labels)\n",
        "tn, fp, fn, tp = con_mat_df.ravel()\n",
        "tpr = tp/(tp+fn)\n",
        "tnr = tn/(tn+fp)\n",
        "\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "print(\"TPR is: \", tpr)\n",
        "print(\"TNR is: \", tnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHsYTUKOJlPT"
      },
      "source": [
        "- going deeper\n",
        "- DL for logistic regression is a bit of an overkill (Ng's slide below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvbdISzP4cZd"
      },
      "source": [
        "## Multiclass classification\n",
        "\n",
        "We now extend logistic regression to multiclas classification using [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "The problem requires us to do three-classes classification using a Softmax function, which can be easily considered as an extension of logistic regression over three (or more) classes. We use the neural network-like implementation as with binary classification.\n",
        "\n",
        "Luckily, Keras provides a [softmax activation function](https://keras.io/api/layers/activations/#softmax-function), which we will use instead of the logistic we previously used.\n",
        "\n",
        "The structure of our network will be similar, but the output goes from a single number to **three** numbers, one per class, and we thus need three nodes:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/softmax_neuron.png\">\n",
        "\n",
        "As a result, the loss function will need to change. Remember, loss represents a measure of how good the predictions are. Previously we used binary_crossentropy, but since now predictions are multiclass we need to change function. Luckily Keras provides a natural extension for the multiclass case with [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH60aH_icjhE"
      },
      "source": [
        "The problem is that our target array `iris.target` is a numeric array. But those numbers we used (0, 1, and 2) do not represent real values. In other words, \"virginica\" is not twice \"versicolor\". Numbers here are used as labels, not as quantities.\n",
        "\n",
        "In fact, to properly train a model the structure of the target array must change to [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). In simple terms, it needs to become a table with one row per sample (150 in total) and one column per class (three in total). Something like:\n",
        "\n",
        "| Setosa | Versicolor | Virginica |\n",
        "|------|------|------|\n",
        "|   0  |   1  |   0  |\n",
        "|   1  |   0  |   0  |\n",
        "|   1  |   0  |   0  |\n",
        "|   0  |   0  |   1  |\n",
        "\n",
        "As you can see the first sample is Versicolor, the second and third are Setosa, the last one is Virginica. Note that there is only a single \"one\" per row.\n",
        "\n",
        "Luckily, it's easy to pass to one-hot encoding using keras function [to_categorical](https://keras.io/api/utils/python_utils/#to_categorical-function):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GS_S3ADGezR"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "iris.data = pd.DataFrame(iris.data, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(iris.target) #converting numpy array -> pandas Series\n",
        "iris.target = iris.target.to_frame() \n",
        "\n",
        "features = iris.data.iloc[:,:]\n",
        "target = iris.target\n",
        "\n",
        "#the \"utils\" subpackage is very useful, take a look to it when you have time\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#converting to categorical\n",
        "target_multi_cat = to_categorical(target)\n",
        "\n",
        "#since everything else is a Pandas dataframe, let's stick to the format\n",
        "#for consistency\n",
        "target_multi_cat = pd.DataFrame(target_multi_cat)\n",
        "\n",
        "#let's take a look\n",
        "print(target_multi_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYXSzKfqdZ9p"
      },
      "source": [
        "### Training and validation sets\n",
        "\n",
        "We are now ready to create our training and validation sets, as done above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LeIcq8GUzO"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "test_pct = 0.2\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "#random_state is used to control class balance between training and test sets (None to switch to random behavior)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size= test_pct, random_state=0)\n",
        "\n",
        "for train_index, val_index in sss.split(features, target_multi_cat):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target_multi_cat.iloc[train_index, :]\n",
        "    target_val     = target_multi_cat.iloc[val_index, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMynbrR4dlqi"
      },
      "source": [
        "Just a quick check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuNyBpZcdn7t"
      },
      "source": [
        "#shapes\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)\n",
        "\n",
        "#number of classes per split\n",
        "print('\\nClasses in train set:')\n",
        "print(target_train.sum())\n",
        "print('\\nClasses in validation set:')\n",
        "print(target_val.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1FwVoczduGB"
      },
      "source": [
        "We have now a balanced dataset, with 40 instances for each class in the training set and 10 in the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96RIRyoAdxTS"
      },
      "source": [
        "### Set up\n",
        "\n",
        "We define here the hyperparameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsNG5urlOOPm"
      },
      "source": [
        "## # Configuration options\n",
        "num_classes = 3\n",
        "input_shape = (features.shape[1],)\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'softmax'\n",
        "loss_function = 'categorical_crossentropy'\n",
        "optimizer_used = 'rmsprop'\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-931xAU7d-ok"
      },
      "source": [
        "### A multiclass model\n",
        "\n",
        "We are now ready to declare our neural network model for multiclass classification: we use `Keras`. The output layer has three units, corresponding to the three classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6KZ6yFNGvpQ"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will \n",
        "#flow like INPUT -> ELABORATION -> OUTPUT.\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes.\n",
        "from keras.layers import Dense\n",
        "# 3-class softmax regression in Keras\n",
        "model_multi = Sequential()\n",
        "model_multi.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n",
        "model_multi.add(Dense(num_classes, activation=output_activation))\n",
        "\n",
        "#compile the model specifying the new multiclass loss\n",
        "model_multi.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImvhF0tfOaqh"
      },
      "source": [
        "print(model_multi.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA2eyImzei27"
      },
      "source": [
        "We see that we have 67 parameters to train: 5 parameters (w1 ... w4, b) times 8 nodes in the hidden layer; 9 parameters (8 units + b) times three nodes in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRn6lZUOiE1"
      },
      "source": [
        "history_multi = model_multi.fit(features_train, target_train, epochs=num_epochs, \n",
        "                     validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqwHBBlMfmT6"
      },
      "source": [
        "plot_loss_history(history_multi, 'Softmax multiclass (200 epochs)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbhLwnMoOl6K"
      },
      "source": [
        "predictions = model_multi.predict(features_val)\n",
        "print(predictions)\n",
        "\n",
        "# predicted_classes = model_multi.predict_classes(features_val)\n",
        "predicted_classes = np.argmax(model_multi.predict(features_val), axis=-1)\n",
        "target_classes = target.iloc[val_index,:].to_numpy()\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes)\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzQwUbZqOq4s"
      },
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CJ8uBqMfy2R"
      },
      "source": [
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes, predicted_classes, normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRmVTlUySgOR"
      },
      "source": [
        "## A side note\n",
        "\n",
        "Neural networks models and deep learning models may be an overkill when applied to a classification problem on 150 samples and 4 features: it is not even guaranteed that they will perform better than simpler machine learning methods like logistic regression. Indeed at times deep learning may perform worse than simpler approaches. \n",
        "As a matter of fact, deep learning is known to work best when the size of the problem is very large (both in terms of amount of data and order of computations needed), and the advent of *Big Data* is exactly one of the drivers behind the rise of deep learning.\n",
        "\n",
        "The Figure below comes from slides by <a href='https://en.wikipedia.org/wiki/Andrew_Ng'>Andrew Ng</a> and illustrates this point: when the amount of data is limited, traditional machine learning methods and small and large neural networks have similar performance. It is only when the size of the problem increases that deep learning shows its potential and consistently outperforms other methods/algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXL0otE9IyX2"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1_Nom99R963AhM30UbbDLg4-u-mPSiX_L\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSS4Mt2NcNeZ"
      },
      "source": [
        "As a matter of fact, a reliable way to get better predictive performance is often to either **train a bigger network** or feed **more data** to it.\n",
        "Eventually you'll hit the limit: i) run out of training examples; or ii) network so big that it is too slow to train "
      ]
    }
  ]
}