{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUy9M38H59ta"
      },
      "source": [
        "# A deep-learning neural network for image recognition\n",
        "We present here a `Python Keras` implementation of a deep learning neural network for image recognition.\n",
        "The publicly available `MNIST` dataset is used.\n",
        "\n",
        "`Keras` is a popular `Python` library for deep learning models:\n",
        "- wrapper for `TensorFlow`\n",
        "- minimalistic\n",
        "- modular\n",
        "- easy to implement\n",
        "\n",
        "The `MNIST` database (Modified National Institute of Standards and Technology database) is a large database of hand-written digits (details and data [here](http://yann.lecun.com/exdb/mnist/)):\n",
        "\n",
        "![mnist](https://drive.google.com/uc?id=1KNK3-8qahQixvL-StpDAs6GoOUAHKSDy)\n",
        "\n",
        "Deep learning consists of neural networks with multiple hidden layers that learn increasingly abstract and complex representations of the input data.\n",
        "For instance, if we train a deep learning model to recognize hand-written digits (images):\n",
        "\n",
        "- the first hidden layers might only learn local edge patterns;\n",
        "- subsequent layers learns more complex representations of the data;\n",
        "- the last layer will classify the image as one of ten digits.\n",
        "\n",
        "For image recognition we use a specific deep learning architecture: **convolutional neural networks** (*CNN*), which assume that input data are images, thereby greatly reducing the number of model parameters to be tuned (more on *CNN's* later in the course).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KuW32WXxarO"
      },
      "source": [
        "## Neural network architecture\n",
        "\n",
        "We saw previously DL models with **dense** neural network architectures: this is a stack of (fully connected) layers in a (deep) sequence.\n",
        "\n",
        "When analysing image data, it turns out that a different neural network architecture is needed: **Convolutional Neural Networks** (abbreviated as **\"CNN\"**). \n",
        "\n",
        "CNN share similarities with Dense Neural Networks: \n",
        "\n",
        "- neurons (units)\n",
        "- layers\n",
        "- weights and biases (to be learned) \n",
        "- loss function (e.g. crossentropy)\n",
        "- optimizer (e.g. `SGD`)\n",
        "\n",
        "Additionally, in CNNs there are also:\n",
        "\n",
        "- Convolutional Layers\n",
        "- Pooling Layers\n",
        "- Flatten Layers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yX0iwczVzv"
      },
      "source": [
        "## Why Convolutional Neural Networks?\n",
        "\n",
        "In Dense Neural Networks all the neurons are connected to each other. Therefore the number of parameters grows very fast. \n",
        "For example, when we have images with 28 by 28 pixels in greyscale, we will end up having 784 (28 x 28 x 1) parameters to learn. \n",
        "However, most images have way more pixels and they are not grey-scaled. For instance, color images in 4K Ultra HD will have 26,542,080 (4096 x 2160 x 3) parameters to learn. \n",
        "\n",
        "Therefore, we can say that Dense Neural Networks are not scalable for image classification. \n",
        "However, in images pixels that are close to each other tend to be correlated: this leads to the idea of **Convolutional Layers** and **Pooling Layers**.\n",
        "\n",
        "Due to the fact that pixels are related to the adjacent and close pixels, convolution allows us to preserve the relationship between different parts of an image. Convolution is basically filtering the image with a smaller pixel filter to decrease the size of the image without losing the relationship between pixels. When we apply convolution to 5x5 image by using a 3x3 filter with 1x1 stride (1-pixel shift at each step) we end up with a 3x3 output (64% decrease in complexity: from 25 to 9 parameters to learn).\n",
        "\n",
        "![cnn](https://drive.google.com/uc?id=1DE0wYYtA8JTPvC4XH3e875mM4yQFT-10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG18PhUOLMFk"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqG72eSnLJYC"
      },
      "source": [
        "We import the necessary libraries to build a DL NN for image recognition:\n",
        "\n",
        "- import the Sequential model type from Keras: linear stack of neural network layers, to be used to build a feed-forward CNN\n",
        "-  import the 'core' layers from Keras: layers that are used in almost any neural network\n",
        "- import the CNN layers from Keras: convolutional layers to train the model on image data\n",
        "- load the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh-2gj8A5WAM"
      },
      "source": [
        "import keras.utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K # needed for image_data_format()\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG7RUXrA52gj"
      },
      "source": [
        "# Input data\n",
        "\n",
        "We load the data from the MNIST dataset, and assign them to the training and testing sets. \n",
        "\n",
        "Image data is generally harder to work with than flat relational data. The MNIST dataset is a beginner-friendly intoduction to working with image data: it contains $70\\,000$ labeled images of handwritten digits. These are grey-scale images, 28 x 28 pixels.\n",
        "\n",
        "The MNIST dataset comprises $60\\,000$ training observations and $10\\,000$ test observations: the function `load_data()` automatically assigns these to the training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaLJe1Vn0lXI"
      },
      "source": [
        "# the data, split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train[0:5000,]\n",
        "y_train = y_train[0:5000]\n",
        "X_test = X_test[0:1000,]\n",
        "y_test = y_test[0:1000]\n",
        "\n",
        "print(\"Size of the training set\")\n",
        "print(X_train.shape)\n",
        "print(\"Size of the test set\")\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgVK5wI4GetE"
      },
      "source": [
        "Data have been split into a **training** and a **testing set**, and within these into a **three-dimensional array** $X$ of **features** (samples x pixels x pixels) and a vector $y$ of labels (0-9 digits).\n",
        "\n",
        "Each record in the 3-D array $X$ is a 28 x 28 matrix of grayscale intensities (1 byte = 8 bits = 0 - 255 values). Grayscale (black-n-white) images only use one color channel. Colour images use three channels (e.g. RGB) and each image (record) is therefore a 3-D matrix (pixels x pixels x 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7wusZdU5FDX"
      },
      "source": [
        "print(\"First training label: \",y_train[0])\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kfxPKriPOjO"
      },
      "source": [
        "By default the matplotlib function `imshow()` uses pseudocolors to plot grayscale images; if you want to display the actual grayscale image, you can specify the color mapping parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFn4vTd3PEwl"
      },
      "source": [
        "print(\"First training label: \",y_train[0])\n",
        "plt.imshow(X_train[0], cmap='gray', vmin=0, vmax=255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbGoAqrV55W5"
      },
      "source": [
        "#Configuration parameters\n",
        "\n",
        "Define model parameters:\n",
        "\n",
        "- batch size: DL models typically do not process the entire dataset at once, rather break it in **batches**\n",
        "- n. of classes: n. of classes to predict (10 digits, in the MNIST problem)\n",
        "- n. of epochs: n. of **iterations** over the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l6_rE5YvYm2"
      },
      "source": [
        "img_rows = 28 #pixels\n",
        "img_cols = 28 #pixels\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "num_epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1i4lpSr9IbD"
      },
      "source": [
        "# Data preprocessing\n",
        "\n",
        "First, we need to explicitly declare the depth of the image representation array: in the case of grayscale images there is only one channel, and this dimension is 1.\n",
        "\n",
        "We use the utility function [image_data_format()](https://keras.io/api/utils/backend_utils#imagedataformat-function) from keras [backend utilities](https://keras.io/api/utils/backend_utils/) to discover the convention ('channels_first' or 'channels_last') of our current system.\n",
        "\n",
        "Depending on the backend (Theano or TensorFlow), the depth dimension is either the first or the last to be declared:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0iC2KdoCyGv"
      },
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(1, X_train.shape[0], img_rows, img_cols)\n",
        "    X_test = X_test.reshape(1, X_test.shape[0], img_rows, img_cols)\n",
        "    input_shape = (1, 28, 28)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "\n",
        "print(\"Modified array dimensions:\")\n",
        "print(X_train.shape) \n",
        "print(input_shape)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvOBH3WoLy6p"
      },
      "source": [
        "We then convert the input data type to `float32` and normalize the data values to the range $[0, 1]$.\n",
        "These are operational modifications necessary to speed up and optimize the calculations.\n",
        "\n",
        "Finally, label vectors are converted to binary class matrices. This serves to convert a vector of numerical digits to a matrix of ten classes per observation, which is a better suited representation for a classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER8vhNco50si"
      },
      "source": [
        "#the \"utils\" subpackage is very useful, take a look to it when you have time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255 #max value of pixel intensity\n",
        "X_test /= 255 #max value of pixel intensity\n",
        "print('x_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices (also known as OHE - One Hot Encoding)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pg9tUjtQX2d"
      },
      "source": [
        "print(y_train[0:4]) ## print first four training examples (labels): which digits are these?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUJzLw46FvK"
      },
      "source": [
        " # Model building\n",
        "\n",
        "We now define our deep-learning **neural network architecture**, and start building our model for image recognition.\n",
        "\n",
        "First, we declare a [sequential model](https://keras.io/guides/sequential_model/), that is a sequence of layers each with one input tensor and one output tensor. \n",
        "Then we add a first convolutional layer ([Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)) to our model; parameters are:\n",
        "\n",
        "- number of convolution filters (n. of kernels to convolve with the input data)\n",
        "- number of rows and columns in each convolution kernel\n",
        "- type of activation function\n",
        "- shape of the input array\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udCTMi2JWaY3"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = Sequential() # topology\n",
        "model.add(\n",
        "          Conv2D(32, kernel_size=(3, 3),\n",
        "          activation='relu',\n",
        "          input_shape=input_shape))\n",
        "\n",
        "print(model.output_shape) ## convolutional \"padding\" (28-(3-1) x 28-(3-1)) + 32 kernels (filters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y68JuKReaLOg"
      },
      "source": [
        "The input shape is (60000, 28, 28, 1): 28 x 28 pixels, times 1 channel (grayscale), per 60,000 training samples.\n",
        "The convolutional output shape is:\n",
        "\n",
        "- None: not yet any samples trained (to be added later)\n",
        "- 26 x 26: convolutional padding (3x3 kernel size $\\rightarrow$ 28-2 x 28-2)\n",
        "- 32: n. of convolutional filters (kernels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC8qRgETmd00"
      },
      "source": [
        "Then we can add more layers to the deep-learning model:\n",
        "\n",
        "- the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) layer is a way to regularize our model to prevent overfitting\n",
        "- [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) is a way to reduce the number of model parameters by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values\n",
        "- a [Dense](https://keras.io/api/layers/core_layers/dense/) layer whose first parameter is the output size of the layer (weights from the Convolution layers must be flattened -made 1-dimensional- before being passed on to the fully connected Dense layer)\n",
        "- the final layer has an output size of 10 (the 10 classes of digits): the activation function here is [softmax](https://keras.io/api/layers/activations/#softmax-function) (the multiclass analog of the logistic function) which returns a probability for each class, e.g. 10% of chance of the sample belonging to class 1, 15% for class 2 and so forth. The sum of all probabilities adds to 100%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wPciJ0ambwV"
      },
      "source": [
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtDwXUoTO7wc"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU9EJQWaPqdf"
      },
      "source": [
        "- first convolutional layer: 3x3 filter x 32 filters +32 bias terms = 320 parameters\n",
        "- second colnvolutional layer: 3x3 filter x 64 filters x 32 units (previous layer) + 64 bias terms = 18496 parameters\n",
        "-  pooling layer: 2x2 --> reduces the dimensionality by half per (\"convolved\") image (from 24x24 to 12x12) \n",
        "- dropout layer: randomly sets some units (neurons) to zero\n",
        "- flatten: transforms 12x12x64 input tensor into a 1-D vector: 12 x 12 x 64 = 9216 output values\n",
        "- dense layers: 128 units x 9216 inputs + 128 bias terms = $1\\,179\\,776$ parameters\n",
        "- output layer: 10 units (classes) x 128 + 10 bias terms = 1290 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF9yHLD4sl4T"
      },
      "source": [
        "### Compiling the model\n",
        "When compiling the model we specify the **loss function** (here: [categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)) and the **optimizer** (here: [Adadelta](https://keras.io/api/optimizers/adadelta/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9COW8p3Z6IBQ"
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKeXuJK86QCJ"
      },
      "source": [
        "# Training the deep-learning model\n",
        "\n",
        "We then fit the model on the training data, specifying:\n",
        "\n",
        "- the batch size\n",
        "- the number of epochs to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv_dGPGu6RWx"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUAxpkz4O8A"
      },
      "source": [
        " # history2 = model.fit(X_train, y_train,\n",
        " #          batch_size=batch_size,\n",
        " #          epochs=10,\n",
        " #          validation_data=(X_test, y_test),\n",
        " #          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TUI5JW8hfg"
      },
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7n-6rZQ6TWk"
      },
      "source": [
        "# Model evaluation\n",
        "\n",
        "We can now measure the performance (in terms of prediction accuracy) of the trained deep-learning model for image recognition. \n",
        "To measure the performance, we applied our trained model to independent test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ZyOafK6UnN"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Rx5ww3ZfsO"
      },
      "source": [
        "### Confusion matrix\n",
        "\n",
        "A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is another way to express the accuracy of your predictions. It's a square matrix, with as many rows (and columns) as your classes. Rows represent *true values* and columns represent *predicted values*. On the main diagonal are thus reported the correct predictions, while off-diagonal elements represent errors.\n",
        "\n",
        "We'll use the [confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function part of [scikit-learn library](https://scikit-learn.org/stable/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHY9mr6OXvDh"
      },
      "source": [
        "#asking our model to return its predictions for the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "#confusion_matrix function requires actual classes labels (expressed as int)\n",
        "#and not probabilities as we handled so far\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "true_classes = y_test.argmax(axis=1)\n",
        "\n",
        "#rows are true values, columns are predicted values, numbering starts from zero\n",
        "import sklearn.metrics\n",
        "con_mat_df = sklearn.metrics.confusion_matrix(true_classes, predicted_classes)\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFXg6obcHl4"
      },
      "source": [
        "Can you spot the most ambiguous, often confounded classes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzmOno5Y8xVf"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFo5oiieMxeK"
      },
      "source": [
        "### Cohen's k\n",
        "\n",
        "For multiclass classification problems we can also use other metrics to measure performance, like for instance `Cohen's kappa` (or `k`) (more info <a href='https://en.wikipedia.org/wiki/Cohen%27s_kappa'>here</a>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odDWwmE5I-x1"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "## training set\n",
        "y_hat_training = model.predict(X_train)\n",
        "y_class_training = y_hat_training.argmax(axis=1)\n",
        "y_true_training = y_train.argmax(axis=1)\n",
        "print(len(y_class_training))\n",
        "k_train = cohen_kappa_score(y_class_training,y_true_training)\n",
        "print(\"Cohen kappa in the training set is: \", k_train)\n",
        "\n",
        "## test set\n",
        "k_test = cohen_kappa_score(predicted_classes,true_classes)\n",
        "print(\"Cohen kappa in the test set is: \", k_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}